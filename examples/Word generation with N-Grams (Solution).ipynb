{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde296bb",
   "metadata": {},
   "source": [
    "# Word generation with N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa74e1b",
   "metadata": {},
   "source": [
    "## 1. Introduction, tool functions and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea9c10",
   "metadata": {},
   "source": [
    "In this \"lab\", we will be trying to generate words.  \n",
    "To do that, we will use the following simple idea: if we get a list of characters, let us say \"welcom\", then we can try to deduct what the next character will be.  \n",
    "To do that, you just have to take a dataset of words/sentences, search for all occurrences of the string \"welcom\", and look at the next character.  \n",
    "We might for instance find $5$ occurrences of \"welcom\", and see that it was followed by \"e\" $4$ times (as in \"welcome\"), and by \"i\" once (as in welcoming).  \n",
    "If our dataset is large enough, we can estimate the probabilities for the next character given a sequence of characters (here in our example the probability that the next character is an \"e\" would be around $80~\\%$ and the probability that it is an \"i\" would be around $20~\\%$).  \n",
    "We can then start with a sequence of characters and keep appending the most likely character to form words that are likely to exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c94895",
   "metadata": {},
   "source": [
    "We start by importing some tools that we'll need.  \n",
    "If you need to install `nltk`, use the following command: `conda install -c anaconda nltk`  \n",
    "If you need to install `seaborn`, use the following command: `conda install -c anaconda seaborn` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba27455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "words = nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553a75f",
   "metadata": {},
   "source": [
    "Here we are importing the dataset of words that we will use to train our model.   \n",
    "- Use list comprehension to create a (numpy) array that contains all words in `all_words` that have a length greater or equal to $2$.\n",
    "- Shuffle this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7546be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = words.words()\n",
    "words_list = np.array([x for x in all_words if len(x) >= 2]) #THIS LINE SHOULD BE ...\n",
    "np.random.shuffle(words_list) #THIS LINE SHOULD BE ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ea49a",
   "metadata": {},
   "source": [
    "`asvoid` is an utilitary function, do not worry too much about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asvoid(arr):\n",
    "    arr = np.ascontiguousarray(arr)\n",
    "    return arr.view(np.dtype((np.void, arr.dtype.itemsize * arr.shape[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac6486",
   "metadata": {},
   "source": [
    "Here, `arr` and `x` are basically just two 1D arrays.  \n",
    "Complete the `find_index` function so that it returns the index of the first element identical in both lists (hint: use `np.nonzero`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4439310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(arr, x):\n",
    "    arr_as1d = asvoid(arr)\n",
    "    x = asvoid(x)\n",
    "    return np.nonzero(arr_as1d == x)[0][0] #THIS LINE SHOULD BE ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f7eeb",
   "metadata": {},
   "source": [
    "To move closer to our goal of generating words, we will need a little bit of vocabulary.  \n",
    "- A *token* is the basic unit that we are trying to predict. For example, we can try to predict the next word, or the next character.  \n",
    "- A *tokenizer* is a function that splits a string into tokens. For instance, in our example we said that a token is a character, so our tokenizer will be a function that splits a string into a list of characters.  \n",
    "\n",
    "Define a tokenizer that splits a string into a list of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d80bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(s:str):\n",
    "    l = [c for c in s] #THIS LINE SHOULD BE ...\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af49adbb",
   "metadata": {},
   "source": [
    "Then, we need to define some parameters for our model.  \n",
    "The first parameter that we need is the number $n$ of past characters we use to predict the next one.  \n",
    "In our starting example, we used $6$ characters (\"welcom\") to predict the $7^{\\text{th}}$. If we instead picked $n=4$, we would have used only the last $4$ characters (\"lcom\") to predict the next one.  \n",
    "\n",
    "Then we need an *alphabet*: the list of valid tokens (so the list of valid characters in our case).  \n",
    "\n",
    "If we get the string \"hello\", what is the most likely next character? Valid answers are probably \"a space\" or \"none\". We will create a special token (a special character) to represent both, called an `end_token`. It will allow our model to end itself the words it will generate, without having to specify a desired length.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9615c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "end_token = '\\x00'\n",
    "alphabet = [c for c in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-\"]\n",
    "alphabet.append(end_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c530b",
   "metadata": {},
   "source": [
    "## 2. The core of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48cfa6b",
   "metadata": {},
   "source": [
    "Now, it is time to implement the core of our model.\n",
    "\n",
    "The idea is the following:\n",
    "- We take a bunch of words.\n",
    "- We look at all $n+1$ characters subsets of each word.\n",
    "- We count how often a given subset appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af64fc2a",
   "metadata": {},
   "source": [
    "But first, we need to do a little bit of formatting on our input words.  \n",
    "\n",
    "Given a word $w$, we need to split it into a list of characters using the tokenizer we defined above, and we need to append an `end_token` to it.  \n",
    "But that's not it yet. We will also prepend $n$ `end_token` to each word: by doing so, our model will also be able to learn what are the most likely first characters.\n",
    "\n",
    "Implement the formatting function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(w):\n",
    "    return [end_token]*n + tokenizer(w) + [end_token] #THIS LINE SHOULD BE ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cc37f8",
   "metadata": {},
   "source": [
    "In order to count how often $n+1$ characters subsets (also called $n+1$ gram) appear, it might be a good idea to have a counter for each of those $n+1$ gram.\n",
    "\n",
    "To do so, we first take all combinations of $n+1$ characters, and then it is your job to create a counter for each combination.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = np.array([np.array(p) for p in itertools.product(alphabet, repeat = n + 1)])\n",
    "counters = np.zeros((combinations.shape[0]), dtype=np.int64) #THIS LINE SHOULD BE ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ebcb4",
   "metadata": {},
   "source": [
    "The following function takes a list of words and should:  \n",
    "- Format each word properly.\n",
    "- Find all its $n+1$ grams (subsets of $n+1$ characters). Reminder: `end_token` is also a character.\n",
    "- Update the counters accordingly (hint: use the function `find_index` that was implemented earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_from_list(l:list[str]): #THIS FUNCTION SHOULD BE ...\n",
    "    for w in l:\n",
    "        t = formatting(w)\n",
    "        for i in range(n, len(t)):\n",
    "            gram = np.array(t[i-n:i+1])\n",
    "            index = find_index(combinations, gram)\n",
    "            counters[index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb232250",
   "metadata": {},
   "source": [
    "Now we should train our model with the database we imported at the beginning.  \n",
    "Just call `update_from_list` with a subset of the dataset (a few hundreds for instance).\n",
    "It will definitely take you some time. A few minutes for a $100$ words, maybe a little bit less than half an hour for a thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_from_list(words_list[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ea2d8",
   "metadata": {},
   "source": [
    "Now our model is trained, fantastic!\n",
    "Before generating words, we can try to give the \"probability\" of a word.  \n",
    "\n",
    "Let us consider an example. If we get the word \"cat\", we might know that the probability that a word starts with \"c\" is $0.10$, then we may know that the probability of having an \"a\" given that we had a \"c\" previously is $0.30$, and finally that the probability of having a \"t\" after \"ca\" is $0.15$.  \n",
    "Thus the probability of the word is $0.10 \\cdot 0.30 \\cdot 0.15 = 0.0045$.\n",
    "\n",
    "First, implement a function that returns the probability of a character given the last $n$ characters.\n",
    "Hint: it should:\n",
    "- Get the number of times the last $n$ characters were followed by `c`.\n",
    "- Get the total number of times the last $n$ characters appeared.\n",
    "- Use both numbers to compute the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a79141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_probability(c, gram): #THIS FUNCTION SHOULD BE ...\n",
    "    gram_plus_c = counters[find_index(combinations, np.array(list(gram) + [c]))]\n",
    "    \n",
    "    index = find_index(combinations, np.array(list(gram) + [alphabet[0]]))\n",
    "    all_grams = counters[index:index+len(alphabet)]\n",
    "    gram_total = np.sum(counters)\n",
    "    \n",
    "    if gram_total == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return gram_plus_c / gram_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ad724",
   "metadata": {},
   "source": [
    "Then use this function to return the probability of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(w:str):\n",
    "    probability = 1\n",
    "    formatted = formatting(w)\n",
    "    for i in range(n, len(formatted)-1): #THIS LOOP SHOULD BE ...\n",
    "        c = formatted[i]\n",
    "        gram = np.array(formatted[i-n:i])\n",
    "        probability *= single_probability(c, gram)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7671184",
   "metadata": {},
   "source": [
    "Try your algorithm with various words. The smaller the dataset you trained on, the bigger the change that it outputs zero so don't be scared.  \n",
    "Try with some short words (and don't forget the list of words is randomized so you don't need to start with an 'a' even if you trained your model only with the beginning of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probability(\"cat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1b647",
   "metadata": {},
   "source": [
    "## 3. Word generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee3a4c",
   "metadata": {},
   "source": [
    "It's now time to generate words!  \n",
    "The basic idea is to generate characters one after the other until the generated character is an end-token.  \n",
    "But we need some sort of randomness to do so, otherwise it'll always be the same word.  \n",
    "What we can do is that, for each $n$-gram (group of $n$-characters), we find the probability distribution for the next character. Then we can use this probability distribution to randomly generate the next character using the last $n$ ones.\n",
    "\n",
    "In order to avoid recomputing many times the same probability distributions, start by computing all of them.\n",
    "\n",
    "Hints: \n",
    "- Don't use the `single_probability` function. Instead, use some numpy operations on the counters array. \n",
    "- `np.sum`, `np.nan_to_num` and `np.reshape` might help you.\n",
    "- We also give you a numpy array containing all combinations of $n$-characters. You will not need it directly, but its length might help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdab70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_characters = np.array([np.array(p) for p in itertools.product(alphabet, repeat = n)])\n",
    "\n",
    "def compute_probabilities(): #THIS FUNCTION SHOULD BE ...\n",
    "    probabilities = counters.reshape((len(n_characters), len(alphabet)))\n",
    "    probabilities = np.nan_to_num(probabilities / probabilities.sum(axis=1, keepdims=True), 0)\n",
    "    return probabilities\n",
    "probabilities = compute_probabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cf7f6",
   "metadata": {},
   "source": [
    "Now implement a function that generates a word. Follow these steps:\n",
    "- Start your word (meaning list of characters) with $n$ `end_tokens`.\n",
    "- Select the probability distribution of the last $n$ characters. (The `n_characters` array given before might help you, and don't forget that you implemented a `find_index` function).\n",
    "- Put it in the `fix_p` function before using it.\n",
    "- Using the fixed probability distribution of the last $n$ characters, generate the next one (using `np.random.choice`).\n",
    "- Continue to do so until the generated character is an `end_token` (compare it to `np.array(end_token)`, otherwise it might not work due to numpy special encodings).\n",
    "- Finally, transform your array of characters into a string and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b125e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(): #THIS FUNCTION SHOULD BE ...\n",
    "    result = [end_token]*n\n",
    "    while True:\n",
    "        index = find_index(n_characters, result[-n:])\n",
    "        choice = np.random.choice(alphabet,p=fix_p(probabilities[index]))\n",
    "        result.append(choice)\n",
    "        if choice == np.array('\\x00'):\n",
    "            break\n",
    "    \n",
    "    return \"\".join(result)\n",
    "\n",
    "def fix_p(p):\n",
    "    if p.sum() == 0:\n",
    "        p = np.ones((len(p))) / len(p)\n",
    "    elif p.sum() != 1.0:\n",
    "        p = p*(1./p.sum())\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd300eb",
   "metadata": {},
   "source": [
    "Now is the fun part, generate some words using the function above. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(generate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba3e03",
   "metadata": {},
   "source": [
    "Congratulations! You implemented an entire algorithm to generate random yet syntactically coherent words! This is not ChatGPT yet, but it is a good start :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21cd560",
   "metadata": {},
   "source": [
    "## 4. To go further ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9f777",
   "metadata": {},
   "source": [
    "Here is the link towards a more advanced implementation of the same algorithm, with a little bit more mathematical details and some cool additional features: https://github.com/josh-freeman/HPshape/blob/main/examples/ngrams.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
